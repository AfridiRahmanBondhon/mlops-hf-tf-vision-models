{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep-diver/mlops-hf-tf-vision-models/blob/main/notebooks/parse_tfrecord.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "fQS5HjRULqNE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "az6tm1zlLpa9",
        "outputId": "43a2c8ce-fcbe-42d5-844b-e1536c99bbde"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-09-26 04:55:06.084487: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2022-09-26 04:55:06.084538: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCS Paths\n",
        "\n",
        "The prepared TFRecords are stored in Google Cloud Storage(GCS). GCS is the equivalent service to AWS S3, but it is provided by Google. \n",
        "- `GCS_PATH_FULL_RESOUTION`: this indicates the GCS path where the full resolution(`500x500`) datasets are stored.\n",
        "- `GCS_PATH_LOW_RESOLUTION`: this indicates the GCS path where the lowered resolution(`256x256`) datasets are stored.\n",
        "\n",
        "We provide two different resolutions of datasets. High resolution images takes a longer time to be used in a number of steps in ML pipeline, so it might be useful to test dedicated services to handle a large amount of data such as [**Dataflow**](https://cloud.google.com/dataflow)."
      ],
      "metadata": {
        "id": "GVmSzsPSLt_G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMA1C5NeLpa-"
      },
      "outputs": [],
      "source": [
        "GCS_PATH_FULL_RESOUTION = \"gs://beans-fullres/tfrecords\"\n",
        "GCS_PATH_LOW_RESOLUTION = \"gs://beans-lowres/tfrecords\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing TFRecords\n",
        "\n",
        "The purpose of this section is to verify that TFRecords are correctly saved and structured as intended. There are few functionalities to note:\n",
        "- data is flattened and stored in a binary format, so we need to decode them into `Tensor` and `reshape` into appropriates shapes. \n",
        "- [`tf.io.parse_single_example`](https://www.tensorflow.org/api_docs/python/tf/io/parse_single_example): It parses a single Example proto(col buffer message) and returns a dict mapping feature keys to `Tensor` and `SparseTensor` values.\n",
        "\n",
        "- [`tf.sparse.to_dense`](https://www.tensorflow.org/api_docs/python/tf/sparse/to_dense): if you are not familiar with `tf.sparse.SparseTensor`, please take a look at the [official document](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor). It basically efficiently and effectively stores values of a Tensor, so it could reduce the number of bytes when saved in external files. But, it should be brought back to a dense `Tensor` which is commonly used in AI/TensorFlow.\n",
        "\n",
        "- after `tf.io.parse_single_example` and `tf.sparse.to_dense`, we still don't know the shape of the returned dense `Tensor`. it is basically just a single dimensional array(Tensor), so we need to reshape appropriately with `tf.reshape`.\n",
        "\n",
        "What `tf.data.TFRecordDataset` does is to create `tf.data` from TFRecord files. "
      ],
      "metadata": {
        "id": "xkcFumvjM4IB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNPMDyk0Lpa_"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 4\n",
        "AUTO = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdO7JZVXLpa_"
      },
      "outputs": [],
      "source": [
        "def parse_tfr(proto):\n",
        "    feature_description = {\n",
        "        \"image\": tf.io.VarLenFeature(tf.float32),\n",
        "        \"image_shape\": tf.io.VarLenFeature(tf.int64),\n",
        "        \"label\": tf.io.VarLenFeature(tf.int64),\n",
        "    }\n",
        "    rec = tf.io.parse_single_example(proto, feature_description)\n",
        "    image_shape = tf.sparse.to_dense(rec[\"image_shape\"])\n",
        "    image = tf.reshape(tf.sparse.to_dense(rec[\"image\"]), image_shape)\n",
        "    label = tf.sparse.to_dense(rec[\"label\"])\n",
        "    return {\"pixel_values\": image, \"label\": label}\n",
        "\n",
        "\n",
        "def prepare_dataset(GCS_PATH=GCS_PATH_FULL_RESOUTION, \n",
        "                    split=\"train\", batch_size=BATCH_SIZE):\n",
        "\n",
        "    if split not in [\"train\", \"val\"]:\n",
        "        raise ValueError(\n",
        "            \"Invalid split provided. Supports splits are: `train` and `val`.\"\n",
        "        )\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset(\n",
        "        [filename for filename in tf.io.gfile.glob(f\"{GCS_PATH}/{split}-*\")],\n",
        "        num_parallel_reads=AUTO,\n",
        "    ).map(parse_tfr, num_parallel_calls=AUTO)\n",
        "\n",
        "    if split == \"train\":\n",
        "        dataset = dataset.shuffle(batch_size * 2)\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkbKjh3gLpbA"
      },
      "source": [
        "### Full Resolution Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPD6ZlP1LpbB",
        "outputId": "4c998a01-8fe5-4b96-e1a7-e5bba06378a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-09-26 04:58:13.399342: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2022-09-26 04:58:13.399401: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2022-09-26 04:58:13.399437: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (codespaces-78e142): /proc/driver/nvidia/version does not exist\n",
            "2022-09-26 04:58:13.400225: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "train_dataset = prepare_dataset()\n",
        "val_dataset = prepare_dataset(split=\"val\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXFooVxsLpbB",
        "outputId": "a84b8d89-246d-486f-d039-3f6bbbabb124"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 500, 500, 3) (4, 1)\n"
          ]
        }
      ],
      "source": [
        "for batch in train_dataset.take(1):\n",
        "    print(batch[\"pixel_values\"].shape, batch[\"label\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM0RKGz_LpbC",
        "outputId": "e6a22814-2b91-4e8d-dfee-3740cacc66cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 500, 500, 3) (4, 1)\n"
          ]
        }
      ],
      "source": [
        "for batch in val_dataset.take(1):\n",
        "    print(batch[\"pixel_values\"].shape, batch[\"label\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Y6ZJsmLpbC"
      },
      "source": [
        "### Low Resolution Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxN4B-FULpbC"
      },
      "outputs": [],
      "source": [
        "train_dataset = prepare_dataset(GCS_PATH_LOW_RESOLUTION)\n",
        "val_dataset = prepare_dataset(GCS_PATH_LOW_RESOLUTION, split=\"val\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiBvqU8ILpbD",
        "outputId": "36f119ef-02e7-4b64-f342-67f943b88228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 256, 256, 3) (4, 1)\n"
          ]
        }
      ],
      "source": [
        "for batch in train_dataset.take(1):\n",
        "    print(batch[\"pixel_values\"].shape, batch[\"label\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVhPABtVLpbD",
        "outputId": "19fc2261-8856-4195-f6cc-c11e6ebaf0e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 256, 256, 3) (4, 1)\n"
          ]
        }
      ],
      "source": [
        "for batch in val_dataset.take(1):\n",
        "    print(batch[\"pixel_values\"].shape, batch[\"label\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmMJBYK7LpbD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.13 ('tfx')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "9707615a70fe419aafb4f3d43d719c18bf8e3fff440c597b227e245d64be09b6"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}